{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15d571db-bd01-480f-a02f-e0d26dea8101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/train_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 191 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "import multiprocessing\n",
    "import json\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "import os, torch, random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "MAX_CPUs = max(1, multiprocessing.cpu_count()-1)\n",
    "\n",
    "pandarallel.initialize(nb_workers = MAX_CPUs , progress_bar=False)\n",
    "    \n",
    "\n",
    "config = {\"max_length\": 1488} # 1656 for CAUSAL with FEEDBACK\n",
    "\n",
    "SEED = 13\n",
    "\n",
    "def seed_everything(seed=13):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab553f57-e7a3-4ebf-a6d0-6b8337baaf5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", use_fast = False)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c94f021f-bbb1-4863-8494-0d762a3617ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_text(row, is_test = False):\n",
    "    inst = row[\"instruction\"].replace(\"###Task Description:\\n\",\"<|system|>\\n\")\n",
    "    inst = inst.replace(\"\\n\\n###The\", \"<|end|>\\n\\n<|user|>\\n###The\")\n",
    "    inst = inst.replace(\"\\n\\n###Feedback: \", \"<|end|>\\n\\n<|assistant|>\")\n",
    "    \n",
    "    return inst\n",
    " \n",
    "    \n",
    "def generate_and_tokenize_prompt(prompt):                                                           \n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length= config[\"max_length\"],\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    return result                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e58604-9a39-4fe7-8071-d9738caed301",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
      "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
      "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
      "3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\n",
      "4. Please do not generate any other opening, closing, and explanations.<|end|>\n",
      "\n",
      "<|user|>\n",
      "###The instruction to evaluate:\n",
      "I'm working on a project that involves creating a user-friendly chatbot for a digital library. The users should be able to ask the chatbot for book recommendations based on their preferences. However, users' preferences can be vague or ambiguous. For instance, a user might say \"I want a book like Harry Potter but different\", \"I liked the character development in Pride and Prejudice, suggest something similar\", or \"Do you have a book that's exciting and thought-provoking but not too difficult to read?\". How can the chatbot handle such ambiguous or vague inputs to provide accurate book recommendations?\n",
      "\n",
      "###Response to evaluate:\n",
      "To handle ambiguous or vague inputs, the chatbot should be able to interpret the user's preferences based on context and keywords. For example, if a user wants a book like Harry Potter but different, the chatbot could suggest fantasy books with different plots or characters. If a user likes character development in Pride and Prejudice, the chatbot could recommend novels with similar themes or well-developed characters.\n",
      "\n",
      "In some cases, the chatbot might misunderstand the user's intent or need to ask for clarification. For instance, if a user asks for a book that's exciting and thought-provoking but not too difficult to read, the chatbot may suggest a thriller novel, but it could also ask the user for more details about their preferences.\n",
      "\n",
      "Overall, the chatbot should aim to provide accurate book recommendations based on the user's input, but it might occasionally misinterpret their preferences or need to ask for more information to provide a suitable suggestion.\n",
      "\n",
      "###Reference Answer (Score 5):\n",
      "To handle ambiguous or vague inputs, the chatbot should be designed with a high level of natural language understanding and processing. This includes the ability to interpret semantics, context, and sentiment in user input.\n",
      "\n",
      "1. **Contextual Understanding:** The chatbot should be able to understand and relate to the context provided by the user. For instance, if a user says they want a book like Harry Potter but different, the chatbot could interpret this as the user wanting a book in the fantasy genre but with a different storyline or writing style. \n",
      "\n",
      "2. **Semantics Interpretation:** If a user mentions they enjoyed the character development in 'Pride and Prejudice', the bot should understand that the user is likely interested in novels with well-rounded, evolving characters and perhaps, a focus on interpersonal relationships and societal norms.\n",
      "\n",
      "3. **Sentiment Analysis:** The chatbot should be able to detect sentiment in the user's input. If a user asks for something 'exciting and thought-provoking but not too difficult to read', the chatbot should understand that the user likely wants a book that is engaging and intellectually stimulating, but not too complex or dense in writing style.\n",
      "\n",
      "In cases where the user's input is too vague or ambiguous, the chatbot should be programmed to ask follow-up questions in a natural and conversational manner. For example, if a user says they want a book like Harry Potter but different, the chatbot could ask, \"Could you please specify in what way you'd like it to be different? Are you looking for a different genre, writing style, or narrative structure?\"\n",
      "\n",
      "By adopting these strategies, the chatbot can effectively handle ambiguity and vagueness in user input, provide accurate responses, and ensure a pleasant user experience.\n",
      "\n",
      "###Score Rubrics:\n",
      "[How well does the model handle ambiguity and vagueness in the user's input?]\n",
      "Score 1: The model cannot handle ambiguous or vague inputs, often providing responses that are irrelevant or nonsensical.\n",
      "Score 2: The model struggles with ambiguous or vague inputs, providing accurate responses some of the time, but often misinterpreting the user's intent.\n",
      "Score 3: The model generally handles ambiguous or vague inputs well, but occasionally misinterprets the user's intent or asks for clarification.\n",
      "Score 4: The model handles ambiguous or vague inputs well, usually providing accurate responses and occasionally asking for clarification when necessary.\n",
      "Score 5: The model expertly handles ambiguous or vague inputs, always interpreting the user's intent correctly and asking for clarification in a natural and conversational manner when necessary.<|end|>\n",
      "\n",
      "<|assistant|>\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_parquet(\"../prometheus_with_contexts.parquet\")\n",
    "\n",
    "data[\"text\"] = data.parallel_apply(create_text, axis = 1)\n",
    "data[\"label\"] = data[\"orig_score\"].parallel_apply(lambda x: int(x)-1)\n",
    "\n",
    "print(data[\"text\"].sample(1).values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43993a92-5b3b-4e3d-bcee-ccb113eab77b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1339.0, 1405.0, 1488.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# leng = data[\"text\"].parallel_apply(lambda x: len(tokenizer(x).input_ids))\n",
    "# np.quantile(leng, 0.95), np.quantile(leng, 0.975), np.quantile(leng, 0.99)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_env",
   "language": "python",
   "name": "train_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
