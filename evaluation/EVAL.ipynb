{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d3a8ee6-eaa7-4adb-8e30-2e803754ae89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TASK = \"REG\" # \"CLASSIF\" # \"CAUSAL\" # \"REG\"\n",
    "BASE_MODEL = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "MAX_LENGTH  = 1656\n",
    "WITH_AUG = True\n",
    "LORA_PATH = \"/home/ec2-user/SageMaker/Judge-LLM/reg/06_05_2024_15_31/results/checkpoint-2368\" # AUG Model\n",
    "# LORA_PATH = \"/home/ec2-user/SageMaker/Judge-LLM/reg/03_05_2024_09_26/results/checkpoint-5187\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "316c6ee1-1a13-4fdc-a75a-f0cda0a9aff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from accelerate import notebook_launcher\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, \n",
    "                          Phi3ForCausalLM, Phi3ForSequenceClassification, pipeline)\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import json, re\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from scipy.stats import pearsonr,spearmanr, kendalltau, mode\n",
    "from sklearn.metrics import (r2_score, \n",
    "                             mean_squared_error,\n",
    "                             root_mean_squared_error,\n",
    "                             mean_absolute_error,\n",
    "                            accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report)\n",
    "\n",
    "\n",
    "def load_model(BASE_MODEL, lora_path, task, device, merge_unload = False):\n",
    "    num_labels = 1 if task == \"REG\" else 5\n",
    "    \n",
    "    if task == \"CAUSAL\":\n",
    "        model = Phi3ForCausalLM.from_pretrained(BASE_MODEL, trust_remote_code=True, \n",
    "                                                          device_map = device, \n",
    "                                                          torch_dtype=torch.bfloat16,\n",
    "                                                          attn_implementation = \"flash_attention_2\", \n",
    "                                                          )\n",
    "        \n",
    "    else:\n",
    "        model = Phi3ForSequenceClassification.from_pretrained(BASE_MODEL, trust_remote_code=True, \n",
    "                                                          device_map = device, \n",
    "                                                          torch_dtype=torch.bfloat16,\n",
    "                                                          attn_implementation = \"flash_attention_2\", \n",
    "                                                              num_labels = num_labels\n",
    "                                                          )\n",
    "        \n",
    "    \n",
    "    if model.config.pad_token_id is None:\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "                                                          \n",
    "                                                          \n",
    "    if lora_path is None: return model.eval()\n",
    "                                        \n",
    "    peft_model = PeftModel.from_pretrained(model,\n",
    "                                          lora_path, \n",
    "                                           device_map = device)\n",
    "    \n",
    "    peft_model = peft_model.eval()\n",
    "    if merge_unload: peft_model = peft_model.merge_and_unload()\n",
    "    \n",
    "    return peft_model\n",
    "\n",
    "\n",
    "def predict(model, tokenizer, test_data:list, task, MAX_LENGTH, BATCH_SIZE):\n",
    "    RESULT = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "            \n",
    "        if task in [\"CLASSIF\", \"REG\"]:\n",
    "            BATCHES = [test_data[i:i + BATCH_SIZE] for i in range(0, len(test_data), BATCH_SIZE)]\n",
    "\n",
    "            for batch in tqdm(BATCHES):\n",
    "                inputs = tokenizer(batch, truncation= True, max_length=MAX_LENGTH, padding=\"max_length\", \n",
    "                           return_tensors = \"pt\").to(model.device)\n",
    "\n",
    "\n",
    "                logits = model(**inputs).logits.cpu().to(torch.float32)\n",
    "\n",
    "                if task == \"CLASSIF\": scores = torch.softmax(logits, dim = 1).argmax(axis = 1).numpy()\n",
    "                elif task == \"REG\": scores = np.clip(logits.squeeze().numpy(), 1,5).tolist()\n",
    "\n",
    "                RESULT.extend(scores)\n",
    "\n",
    "        else:\n",
    "            for text in tqdm(test_data):\n",
    "                inputs = tokenizer(text, truncation = True, max_length = MAX_LENGTH, padding=\"max_length\", \n",
    "                           return_tensors = \"pt\").to(model.device)\n",
    "\n",
    "                predicted_tokens = model.generate(**inputs, max_new_tokens = 256, do_sample = False, use_cache = True,\n",
    "                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                pad_token_id = tokenizer.pad_token_id)[0]\n",
    "\n",
    "                \n",
    "                generated_text = tokenizer.decode(predicted_tokens, skip_special_tokens = True)\n",
    "                \n",
    "                try: pred_score = float(re.findall(r\"\\[RESULT\\] \\w\",generated_text)[0].replace(\"[RESULT]\",\"\").strip())\n",
    "                except: pred_Score = -1\n",
    "                \n",
    "                RESULT.append([pred_score])\n",
    "\n",
    "\n",
    "    return RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "810497de-6407-488b-824d-79b1921c55b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast = False)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1fba74-9992-43be-a776-f5bddef83941",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "### With and without Augmented Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1029f8d2-22ef-4e31-b23d-cec28a5756a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys_ref = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate and a reference answer that gets a score of 5 are given.\n",
    "1. Write a detailed feedback that assess the quality of the response.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You can refer to the reference answer (which has a perfect score of 5) to get the idea for scoring.\n",
    "3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\n",
    "4. Please do not generate any other opening, closing, and explanations.\"\"\"\n",
    "\n",
    "sys_rubric = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\n",
    "4. Please do not generate any other opening, closing, and explanations.\"\"\"\n",
    "\n",
    "sys_only = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it) and a response to evaluate are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5.\n",
    "3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\n",
    "4. Please do not generate any other opening, closing, and explanations.\"\"\"\n",
    "\n",
    "sys_ref_rubric = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general. You can also refer to the reference answer (which has a perfect score of 5) to get the idea for scoring.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\n",
    "4. Please do not generate any other opening, closing, and explanations.\"\"\"\n",
    "\n",
    "sys_map_prom = {\"sys_only\":sys_only, \"ref_only\":sys_ref, \"rubric_only\":sys_rubric, \"ref_rub\": sys_ref_rubric}\n",
    "\n",
    "\n",
    "def create_augmented_prompt(inst:str):\n",
    "    inst = inst[inst.find(\"###The instruction to evaluate\"):]\n",
    "    if \"###Reference Answer\" not in inst:\n",
    "        sys_prompt = sys_map_prom[\"rubric_only\"]\n",
    "        que_ans = \"<|user|>\\n\" + inst.replace(\"###Feedback:\", \"\").strip()\n",
    "        \n",
    "        prompt = \"<|system|>\\n\" + sys_prompt + \"<|end|>\\n\\n\" + que_ans + \"<|end|>\\n\\n<|assistant|>\\n\"\n",
    "        return prompt.replace(\"###Feedback:\", \"\").strip('\"')\n",
    "    \n",
    "\n",
    "\n",
    "def create_test_prompt(text):\n",
    "    if WITH_AUG and \"###Reference Answer\" not in text:  return create_augmented_prompt(text)\n",
    "    \n",
    "    text = text.strip(\",\\n\")\n",
    "    inst = text.replace(\"###Task Description:\\n\",\"<|system|>\\n\")\n",
    "    inst = inst.replace(\"\\n\\n###The\", \"<|end|>\\n\\n<|user|>\\n###The\")\n",
    "    inst = inst.replace(\"\\n\\n###Feedback:\", \"<|end|>\\n\\n<|assistant|>\\n\")\n",
    "    \n",
    "    return inst.replace(\"###Feedback:\", \"\").strip('\"')\n",
    "\n",
    "\n",
    "\n",
    "def create_judgelm_panda_prompt(que, ans, ref = \"\"):\n",
    "    que = str(que)\n",
    "    ans = str(ans)\n",
    "    ref = str(ref)\n",
    "    \n",
    "    sys_prompt = sys_map_prom[\"ref_only\"] if len(ref) else sys_map_prom[\"sys_only\"]\n",
    "    \n",
    "    que = \"###The instruction to evaluate:\\n\"+ que.strip() + \"\\n\\n\"\n",
    "    ans = \"###Response to evaluate:\\n\" + ans.strip()\n",
    "    \n",
    "    if len(ref): ans += (\"\\n\\n###Reference Answer (Score 5):\\n\" + ref.strip())\n",
    "    \n",
    "    return \"<|system|>\\n\" + sys_prompt + \"<|end|>\\n\\n\" + que + ans + \"<|end|>\\n\\n<|assistant|>\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438d65f-f2f7-4972-a20c-7571a21769e1",
   "metadata": {},
   "source": [
    "### Prometheus 1,2 Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2fb793-cd15-418f-a321-9e491d912f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feedback_ood = pd.read_json(\"feedback_collection_ood_test.json\") # GPT-4 only. No Human scores . Absolute || REF Ans\n",
    "feedback_ood[\"prompt\"] = feedback_ood[\"instruction\"].apply(create_test_prompt)\n",
    "\n",
    "feedback_test = pd.read_json(\"./feedback_collection_test.json\") # GPT-4 only. No Human scores . Absolute || REF Ans\n",
    "feedback_test[\"prompt\"] = feedback_test[\"instruction\"].apply(create_test_prompt)\n",
    "\n",
    "vicuna = pd.read_json(\"./vicuna_eval.json\") # Absolute. GPT-4 Scores only. Absolute || REF Ans\n",
    "vicuna[\"prompt\"] = vicuna[\"instruction\"].apply(create_test_prompt)\n",
    "\n",
    "flask = pd.read_json(\"./flask_eval.json\") # Absolute. Human + GPT-4 scores\n",
    "flask[\"prompt\"] = flask[\"instruction\"].apply(create_test_prompt)\n",
    "\n",
    "pref_coll = pd.read_json(\"./preference_collection_ood_test.json\") # REF + RUB || Relative || GPT-4, I guess\n",
    "pref_coll[\"chosen_prompt\"] = pref_coll[\"chosen_instruction\"].apply(create_test_prompt)\n",
    "pref_coll[\"rejected_prompt\"] = pref_coll[\"rejected_instruction\"].apply(create_test_prompt)\n",
    "\n",
    "alpaca = pd.read_json(\"alpaca_eval.json\") # Human + Relative + Rubric + No REF\n",
    "alpaca[\"chosen_prompt\"] = alpaca[\"chosen_instruction\"].apply(create_test_prompt)\n",
    "alpaca[\"rejected_prompt\"] = alpaca[\"rejected_instruction\"].apply(create_test_prompt)\n",
    "\n",
    "hhh = pd.read_json(\"./hhh_alignment_eval.json\") # Human. Relative. No TIES. Human preference\n",
    "hhh[\"chosen_prompt\"] = hhh[\"chosen_instruction\"].apply(create_test_prompt)\n",
    "hhh[\"rejected_prompt\"] = hhh[\"rejected_instruction\"].apply(create_test_prompt)\n",
    "\n",
    "mt_human = pd.read_json(\"./mt_bench_human_judgement_eval.json\") # Relative WITH ties. Human Preference\n",
    "mt_human[\"chosen_prompt\"] = mt_human[\"chosen_instruction\"].apply(create_test_prompt)\n",
    "mt_human[\"rejected_prompt\"] = mt_human[\"rejected_instruction\"].apply(create_test_prompt)\n",
    "\n",
    "# auto_j = pd.read_json(\"./autoj_pairwise.json\", lines = True) # We'll see later\n",
    "\n",
    "# mt_eval = pd.read_json(\"./mt_bench_eval.json\", lines = True) # NO Scores for anything. Not using it\n",
    "\n",
    "\n",
    "DATA_MAP = {\"feedback_ood\":feedback_ood, \"feedback_test\":feedback_test, \"vicuna\":vicuna, \"flask\":flask, \n",
    " \"pref_coll\":pref_coll, \"alpaca\":alpaca, \"hhh\":hhh, \"mt_human\":mt_human}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679da6eb-d633-4e78-88b2-002499acf315",
   "metadata": {},
   "source": [
    "### JudgeLM Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7534f3bb-4072-4f0c-a511-2a0926c566cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "judge_que_ans = pd.read_json(\"./judgelm_val_5k.jsonl\", lines = True)\n",
    "judge_review = pd.read_json(\"./judgelm_val_5k_gpt4.jsonl\", lines = True)\n",
    "judge_review_with_ref = pd.read_json(\"./judgelm_val_5k_gpt4_with_reference.jsonl\", lines = True)\n",
    "judge_ref_ans = pd.read_json(\"./judgelm_val_5k_references.jsonl\", lines = True)\n",
    "\n",
    "judgelm = pd.merge(judge_que_ans, judge_ref_ans, on = \"question_id\").merge(judge_review, on = \"question_id\").merge(judge_review_with_ref, on = \"question_id\").drop(\n",
    "    [\"score_x\", \"review_id_x\", \"review_id_y\", \"metadata_x\", \"answer1_id_x\", \"answer2_id_x\", \"reviewer_id_x\", \"answer1_id_y\", \"answer2_id_y\", \"reviewer_id_y\", \"metadata_y\", \"metadata_y\", \n",
    "     \"answer1_model_id\", \"answer2_model_id\", \"answer1_metadata\", \"answer2_metadata\", \"question_body_y\"], axis = 1).rename(\n",
    "         columns = {\"score_y\": \"score\", \"score\": \"score_with_ref\"}\n",
    "     )\n",
    "\n",
    "judgelm[\"prompt_1\"] = judgelm.apply(lambda row: create_judgelm_panda_prompt(row[\"question_body_x\"], row[\"answer1_body\"]), axis = 1)\n",
    "judgelm[\"prompt_2\"] = judgelm.apply(lambda row: create_judgelm_panda_prompt(row[\"question_body_x\"], row[\"answer2_body\"]), axis = 1)\n",
    "\n",
    "judgelm[\"prompt_1_ref\"] = judgelm.apply(lambda row: create_judgelm_panda_prompt(row[\"question_body_x\"], row[\"answer1_body\"], row[\"reference\"][\"text\"]), axis = 1)\n",
    "judgelm[\"prompt_2_ref\"] = judgelm.apply(lambda row: create_judgelm_panda_prompt(row[\"question_body_x\"], row[\"answer2_body\"], row[\"reference\"][\"text\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6bfe6b-7d7e-4f0e-a3ec-c304033b75e6",
   "metadata": {},
   "source": [
    "### PandaLM Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dec63b2-15f3-4da0-852c-602810d96190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "panda = pd.read_json(\"./pandalm_testset-v1.json\")\n",
    "\n",
    "def get_agreement(row):\n",
    "    lis = row[['annotator1', 'annotator2', 'annotator3']].astype(int).values.tolist()\n",
    "    if len(set(lis)) == 3: return \"DROP\"\n",
    "    return mode(lis).mode\n",
    "    \n",
    "\n",
    "panda[\"winner\"] = panda.apply(get_agreement, axis = 1)\n",
    "\n",
    "panda[\"prompt_1\"] = panda.apply(lambda row: create_judgelm_panda_prompt(\n",
    "    row[\"instruction\"] + \"\\n\" + row[\"input\"], row[\"response1\"], \"\"), axis = 1)\n",
    "\n",
    "panda[\"prompt_2\"] = panda.apply(lambda row: create_judgelm_panda_prompt(\n",
    "    row[\"instruction\"] + \"\\n\" + row[\"input\"], row[\"response2\"], \"\"), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47269c58-5c1f-43d0-8b14-ec5bc13444d6",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae36074c-1baa-46d3-926f-fcefc3504375",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]\n",
      "Some weights of Phi3ForSequenceClassification were not initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(BASE_MODEL = BASE_MODEL, \n",
    "                   lora_path = LORA_PATH, \n",
    "                   task = TASK, device = \"cuda:0\", merge_unload = True)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb941e-d5ac-4e21-9b13-0b85e54512fb",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a12bb120-cbca-40f2-b536-aa4a307d9551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_df(df, model, tokenizer, task, MAX_LENGTH, BATCH_SIZE, return_scores = False):\n",
    "    test_data = df.copy(deep = True)\n",
    "    eval_type = \"relative\" if \"chosen_prompt\" in test_data.columns else \"absolute\"\n",
    "    \n",
    "    human, gpt = [], []\n",
    "    if eval_type == \"absolute\":\n",
    "        texts = test_data[\"prompt\"].values.tolist()\n",
    "        \n",
    "        try:\n",
    "            gpt_4 = [np.mean(i) for i in test_data[\"gpt4_score\"].values]\n",
    "            human = [np.mean(i) for i in test_data[\"human_score\"].values]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "        predictions = predict(model, tokenizer, texts, task = TASK, MAX_LENGTH = MAX_LENGTH, BATCH_SIZE = 7)\n",
    "        pred_labels = [float(i) for i in predictions]\n",
    "        \n",
    "        if return_scores: return pred_labels\n",
    "        \n",
    "        results = {\"GPT\":{}, \"Human\": {}}\n",
    "        \n",
    "        if gpt_4:\n",
    "            results[\"GPT\"] = {\"Pearson_r\": round(pearsonr(gpt_4, pred_labels).statistic, 2), \n",
    "                           \"Spearman_r\": round(spearmanr(gpt_4, pred_labels).statistic, 2) , \n",
    "                           \"Kendall_tau\": round(kendalltau(gpt_4, pred_labels).statistic, 2),\n",
    "                           \"R_2\": round(r2_score(gpt_4, pred_labels), 2),\n",
    "                           \"MSE\": round(mean_squared_error(gpt_4, pred_labels), 2), \n",
    "                           \"MAE\": round(mean_absolute_error(gpt_4, pred_labels), 2) \n",
    "                          }\n",
    "    \n",
    "        if human:\n",
    "            results[\"Human\"] =  {\"Pearson_r\": round(pearsonr(human, pred_labels).statistic, 2), \n",
    "                                 \"Spearman_r\": round(spearmanr(human, pred_labels).statistic, 2) , \n",
    "                                   \"Kendall_tau\": round(kendalltau(human, pred_labels).statistic, 2),\n",
    "                                   \"R_2\": round(r2_score(human, pred_labels), 2),\n",
    "                                   \"MSE\": round(mean_squared_error(human, pred_labels), 2), \n",
    "                                   \"MAE\": round(mean_absolute_error(human, pred_labels), 2) }\n",
    "                   \n",
    "    \n",
    "    else:\n",
    "        text_chosen = test_data[\"chosen_prompt\"].values.tolist()\n",
    "        text_rejected = test_data[\"rejected_prompt\"].values.tolist()\n",
    "        \n",
    "        predictions_chosen = predict(model, tokenizer, text_chosen, task = TASK, MAX_LENGTH = MAX_LENGTH, BATCH_SIZE = 7)\n",
    "        pred_labels_chosen = np.array([float(i) for i in predictions_chosen])\n",
    "        \n",
    "        predictions_rejected = predict(model, tokenizer, text_rejected, task = TASK, MAX_LENGTH = MAX_LENGTH, BATCH_SIZE = 7)\n",
    "        pred_labels_rejected = np.array([float(i) for i in predictions_rejected])\n",
    "        \n",
    "        if return_scores: return (predictions_chosen, predictions_rejected)\n",
    "        results = {\"Accuracy\": round((pred_labels_chosen > pred_labels_rejected).mean(), 2)}\n",
    "        \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46baa733-3721-4731-bb68-3b3c9a8f2f00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'human_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [04:10<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'human_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [04:09<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'human_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [01:20<00:00,  1.74s/it]\n",
      "100%|██████████| 286/286 [08:16<00:00,  1.74s/it]\n",
      "100%|██████████| 286/286 [08:20<00:00,  1.75s/it]\n",
      "100%|██████████| 286/286 [08:19<00:00,  1.75s/it]\n",
      "100%|██████████| 371/371 [10:31<00:00,  1.70s/it]\n",
      "100%|██████████| 371/371 [10:30<00:00,  1.70s/it]\n",
      "100%|██████████| 32/32 [00:53<00:00,  1.69s/it]\n",
      "100%|██████████| 32/32 [00:53<00:00,  1.69s/it]\n",
      "100%|██████████| 205/205 [05:49<00:00,  1.70s/it]\n",
      "100%|██████████| 205/205 [05:48<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "final_json = {}\n",
    "\n",
    "for (data_name, val_df) in DATA_MAP.items():\n",
    "    met = evaluate_df(val_df, model, tokenizer, task = TASK, MAX_LENGTH = MAX_LENGTH, BATCH_SIZE = 7)\n",
    "    \n",
    "    final_json[data_name] = met\n",
    "    \n",
    "    \n",
    "final_json[\"metadata\"] = {\"lora_model\": LORA_PATH, \"Base_model\": BASE_MODEL}\n",
    "with open(f\"./eval_results/{str(datetime.now())}_{TASK}_AUG-{WITH_AUG}_LEN-{MAX_LENGTH}.json\", \"w+\") as f: json.dump(final_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1d6de7-b890-4b2f-beba-510d63f0690a",
   "metadata": {},
   "source": [
    "## JudgeLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ce5193d-85fe-4195-b99d-8cd69d2c5ece",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt4_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 715/715 [20:02<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt4_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 715/715 [20:02<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt4_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 715/715 [20:08<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt4_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 715/715 [20:09<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "j_pred_scores = {\"prompt_1\": [], \"prompt_2\": [], \"prompt_1_ref\": [], \"prompt_2_ref\": []}\n",
    "\n",
    "for col_name in j_pred_scores.keys():\n",
    "    tmp = judgelm.copy(deep = True)\n",
    "    tmp[\"prompt\"] = tmp[col_name]\n",
    "    \n",
    "    judgelm[f\"{col_name}_pred_score\"] = (np.array(evaluate_df(tmp, model, tokenizer, task = TASK, MAX_LENGTH = MAX_LENGTH, \n",
    "                                                       BATCH_SIZE = 7, return_scores = True)) *2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "856d1d4c-f23f-4572-ab9f-d7d4825aced7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_acc(row, score1_col, score2_col, gt_score_col):\n",
    "    s1 = row[score1_col]\n",
    "    s2 = row[score2_col]\n",
    "    gt_s1, gt_s2 = row[gt_score_col]\n",
    "    \n",
    "    if (gt_s1 > gt_s2): \n",
    "        if s1 > s2: return True\n",
    "    \n",
    "    elif (gt_s1 < gt_s2): \n",
    "        if s1 < s2: return True\n",
    "    \n",
    "    else: \n",
    "        if round(s1, 0) == round(s2, 0): return True\n",
    "    \n",
    "    return False  \n",
    "\n",
    "round(judgelm.apply(lambda row: calculate_acc(row, \"prompt_1_pred_score\",  \"prompt_2_pred_score\", \"score\"),axis = 1).mean(), 2)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92865c84-542b-4d41-bb43-0ad2068b3df2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(judgelm.apply(lambda row: calculate_acc(row, \"prompt_1_ref_pred_score\",  \"prompt_2_ref_pred_score\", \"score_with_ref\"), axis = 1).mean(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb918fc-e4ac-4455-8737-f2a750e4276a",
   "metadata": {},
   "source": [
    "## PandaLM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "906b2da3-7a67-412f-b59b-1317b8514d68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt4_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [04:00<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'gpt4_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [03:59<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "tmp = panda.copy(deep = True)\n",
    "tmp[\"prompt\"] = tmp[\"prompt_1\"]\n",
    "score_1 = evaluate_df(tmp, model, tokenizer, task = TASK, MAX_LENGTH = MAX_LENGTH, BATCH_SIZE = 7, return_scores = True)\n",
    "\n",
    "del tmp \n",
    "\n",
    "tmp = panda.copy(deep = True)\n",
    "tmp[\"prompt\"] = tmp[\"prompt_2\"]\n",
    "score_2 = evaluate_df(tmp, model, tokenizer, task = TASK, MAX_LENGTH = MAX_LENGTH, BATCH_SIZE = 7, return_scores = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc3c9ede-aa21-479a-bb8b-9c842d97dca3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panda[\"prompt_1_pred\"] = score_1\n",
    "panda[\"prompt_2_pred\"] = score_2\n",
    "\n",
    "def panda_acc(row):\n",
    "    winner = int(row[\"winner\"])\n",
    "    \n",
    "    ps1 = row[\"prompt_1_pred\"]\n",
    "    ps2 = row[\"prompt_2_pred\"]\n",
    "    \n",
    "    if winner == 0: # both\n",
    "        if round(ps1, 0) == round(ps2, 0): return True\n",
    "    elif winner == 1:\n",
    "        if ps1 > ps2: return True\n",
    "    elif winner == 2:\n",
    "        if ps1 < ps2: return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "round(panda.apply(panda_acc, axis = 1).mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879d0ab-1e9f-4b7e-8b1d-5c62324bd1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_env",
   "language": "python",
   "name": "train_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
