Here is list of all the 75+ LLM Evaluation methods, github repos, tools, blogs I could find (till Nov, 2023) for LLM Evaluation

**Order is random**
#

## Repo / Tools:

* [Blog by Lilian](https://lilianweng.github.io/posts/2024-07-07-hallucination/#anti-hallucination-methods)
* [Repo on Lots and Lots of material on evaluation by EdinburgNLP](https://github.com/EdinburghNLP/awesome-hallucination-detection)
* [Rajiv Shah's Repo on LLM Evaluation](https://github.com/rajshah4/LLM-Evaluation)
* [Harpreet's Repo using Langchain to Evaluate Models: Session 7](https://github.com/harpreetsahota204/langchain-zoomcamp/tree/main/session-7)
* [RAGAS](https://github.com/explodinggradients/ragas)
* [Giskard - Test LLMs](https://colab.research.google.com/github/giskard-ai/giskard/blob/main/docs/getting_started/quickstart/quickstart_llm.ipynb) 
* [Auto Evaluator](https://github.com/rlancemartin/auto-evaluator)
* [ReLM](https://github.com/mkuchnik/relm)
* [Tru Lens](https://github.com/truera/trulens)
* [Guard Rails](https://shreyar.github.io/guardrails/)
* [Nemo Guard Rails](https://github.com/NVIDIA/NeMo-Guardrails)
* [DeepEval](https://github.com/confident-ai/deepeval)
* [PromptFoo - Prompt Evals](https://github.com/promptfoo/promptfoo)
* [Thumb - Prompt Testing](https://github.com/hammer-mt/thumb)
* [Prompt Injection Protection](https://github.com/protectai/rebuff)
* [PromptBench](https://github.com/microsoft/promptbench/tree/main)
* [Fact Checker](https://github.com/jagilley/fact-checker) 
* [LangTest](https://github.com/JohnSnowLabs/langtes)
* [Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)
* [Outlines](https://github.com/outlines-dev/outlines)
* [Lakera (Not fully open sourced)](https://www.lakera.ai/)
* [SmartLLMChain](https://python.langchain.com/docs/use_cases/more/self_check/smart_llm)
* [LLMCheckerChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm_checker.base.LLMCheckerChain.html)
* [LLMInformationExtraction Notebook](https://github.com/fhuthmacher/LLMevaluation/blob/main/LLMInformationExtraction.ipynb)
* [Chain of Thought Prompting - Material 1](https://learnprompting.org/docs/intermediate/chain_of_thought)
* [Chain of Thought Prompting - Material 2](https://www.promptingguide.ai/techniques/cot)
* [Tree of Thought Prompting by Princeton NLP](https://github.com/princeton-nlp/tree-of-thought-llm)
* [Tree of Thought Prompting Material 2](https://github.com/kyegomez/tree-of-thoughts)
* [LLM - Eval Survey](https://github.com/MLGroupJLU/LLM-eval-survey)
* [LLM Eval Comprehensive survey paper (111 pages ðŸ™‚ )](https://arxiv.org/pdf/2310.19736.pdf)
* [Verify CoT](https://github.com/lz1oceani/verify_cot)
* [LLM - Augmentor](https://github.com/pengbaolin/LLM-Augmenter)
* [LangChain Different Criteria](https://github.com/philschmid/evaluate-llms/blob/main/notebooks/01-getting-started.ipynb)

## Some Models:
* [Prometheus -2](https://huggingface.co/prometheus-eval/prometheus-7b-v2.0)
* [Vectera](https://huggingface.co/vectara/hallucination_evaluation_model)
* [JudgeLM](https://huggingface.co/BAAI/JudgeLM-33B-v1.0)
* [How close is LLM to Humans](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection) 

## Some Papers:
* [Check your facts and try again](https://arxiv.org/abs/2302.12813)
* [Researching and Revising What Language Models Say](https://arxiv.org/abs/2210.08726)
* [Fact-Checking Complex Claims with Program-Guided Reasoning](https://aclanthology.org/2023.acl-long.386.pdf)
* [Repo + Paper -> SAC: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency](https://github.com/intuit/sac3)
* [Hallucination detection: Robustly discerning reliable answers in Large Language Models](https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=9467&context=sis_research)

## Blog, Video Resources:
* [victordibia](https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination)
* [Evaluation, Measurements and Some Solutions](https://amatriain.net/blog/hallucinations)
* [Kellton - Techniques](https://www.kellton.com/kellton-tech-blog/generative-ai-hallucinations-revealing-best-techniques)
* [FLARE](https://thenewstack.io/3-ways-to-stop-llm-hallucinations/)
* [Seminar by Galileo and Deeplearning.ai](https://www.youtube.com/live/u1pNrsR1txA?si=9hI4FeRExKSGMZK7)
* [Galileo Blog on framework to detect and reduce hallucinations](https://www.rungalileo.io/blog/a-framework-to-detect-llm-hallucinations)
* [Fixing Hallucinations](https://betterprogramming.pub/fixing-hallucinations-in-llms-9ff0fd438e33)
* [Chain of Verification for Detecting Hallucinations](https://medium.com/@raphael.mansuy/making-ai-hallucinate-less-with-chain-of-verification-2e27682e842c )
* [MLFlow Blog](https://www.databricks.com/blog/announcing-mlflow-28-llm-judge-metrics-and-best-practices-llm-evaluation-rag-applications-part)
* [DeepChecks (Paid + BETA)](https://deepchecks.com/solutions/llm-validation/)
* [LLamaIndex + TruLENS](https://truera.com/build-and-evaluate-llm-apps-with-llamaindex-and-trulens/)
* [Scale](https://scale.com/blog/evaluating-llm-agent-performance)
* [Medium: testing-large-language-models-like-we-test-software](https://towardsdatascience.com/testing-large-language-models-like-we-test-software-92745d28a359)
* [V7 Blog](https://www.v7labs.com/blog/how-to-compare-ai-models-with-benchllm)
* [Microsoft Blog](https://www.microsoft.com/en-us/research/group/deep-learning-group/articles/check-your-facts-and-try-again-improving-large-language-models-with-external-knowledge-and-automated-feedback/)
* [RELM](https://blog.ml.cmu.edu/2023/06/05/validating-large-language-models-with-relm/)
* [LLM Eval](https://medium.com/@sheldon88/evaluation-on-llms-4914215459d7)

## Guiding Outputs:
* [Format Enforcer](https://github.com/noamgat/lm-format-enforcer)
* [Guidance](https://github.com/guidance-ai/guidance)
* [Outlines](https://github.com/outlines-dev/outlines)
* [jsonFormer](https://github.com/1rgs/jsonformer)
